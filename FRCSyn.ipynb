{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.9.3\n",
      "GPUs Available:  1\n",
      "0.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"GPUs Available: \", len(gpu))\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import utils\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import mtcnn\n",
    "print(mtcnn.__version__)\n",
    "import cv2\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shutil\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as Img\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.000 Imagens\n",
    "## 5.000 conjuntos/Inputs  \n",
    "Black(1000):               \n",
    "500 Mulheres | 500 Homens: 250 iguais, 250 diferentes de cada   \n",
    "\n",
    "White(1000):            \n",
    "500 Mulheres | 500 Homens: 250 iguais, 250 diferentes de cada  \n",
    "\n",
    "Indianos(1000):       \n",
    "500 Mulheres | 500 Homens: 250 iguais, 250 diferentes de cada  \n",
    "\n",
    "Asiaticos(1000):          \n",
    "500 Mulheres | 500 Homens: 250 iguais, 250 diferentes de cada   \n",
    "\n",
    "Outro(1000):                    \n",
    "500 Mulheres | 500 Homens: 250 iguais, 250 diferentes de cada  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggface2 = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg', weights='vggface')\n",
    "\n",
    "# inputs = vggface2.inputs\n",
    "# outputs = vggface2.output\n",
    "# model2 = Model(inputs, outputs)\n",
    "# model2.compile()\n",
    "# model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Imagem Esquerda (InputLayer)   [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Imagem Direita (InputLayer)    [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " vggface_resnet50 (Functional)  (None, 2048)         23561152    ['Imagem Esquerda[0][0]',        \n",
      "                                                                  'Imagem Direita[0][0]']         \n",
      "                                                                                                  \n",
      " L1_layer (Lambda)              (None, 2048)         0           ['vggface_resnet50[0][0]',       \n",
      "                                                                  'vggface_resnet50[1][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,561,152\n",
      "Trainable params: 23,508,032\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Definição da Siamese Convolutional Neural Network\n",
    "vggface = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg', weights='vggface')\n",
    "img_A = layers.Input((224, 224, 3), name = \"Imagem Esquerda\")\n",
    "img_B = layers.Input((224, 224, 3), name = \"Imagem Direita\")\n",
    "features_A = vggface(img_A)\n",
    "features_B = vggface(img_B)\n",
    "L1_layer = layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]), name = \"L1_layer\")\n",
    "L1_distance = L1_layer([features_A, features_B])\n",
    "model = Model(inputs = [img_A,img_B], outputs = L1_distance)\n",
    "# for layer in model.layers[:-3]:\n",
    "#     layer.trainable = False\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formato dos dados\n",
    "### Como precisamos de duas imagens como input, teremos:\n",
    "[X_train_A, X_train_B] e Y_train\n",
    "### dessa forma:\n",
    "[img_A, img_B] e AreTheyTheSame?\n",
    "### Shape:\n",
    "[(5000, 224, 224, 3), (5000, 224, 224, 3)] e (5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a = []\n",
    "x_train_b = []\n",
    "y_train = []\n",
    "amount = 504   #per ethnicity\n",
    "# no treino pegaremos 504 imagens por etnia de modo que:\n",
    "# 0-251 teremos comparacao entre diferentes imagens da mesma pessoa.            Ex: 0[0] com 0[1]   | 1[0] com 1[1] e assim por diante...\n",
    "# 252-503 faremos comparacao entre diferentes imagens de pessoas diferentes.    Ex: 0[0] com 252[0] | 1[0] com 253[0] e assim por diante...\n",
    "def trainBuild(x_train_a, x_train_b, y_train, amount, path):\n",
    "    #print(\"Size of x_train = \",len(x_train[0]))\n",
    "    all_folders = [f for f in os.listdir(path)]\n",
    "    #print(all_folders)\n",
    "    for folder in all_folders[:int(amount/2)]:\n",
    "        counter = 0\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png'))]\n",
    "\n",
    "        #pegaremos a primeira imagem da pasta da pessoa\n",
    "        image_A = Image.open(folder_path+\"/\"+str(image_files[0]))\n",
    "        image_A = image_A.resize((224,224))\n",
    "        img_A_array = np.array(image_A)\n",
    "\n",
    "        #pegaremos a segunda imagem da pasta da pessoa \n",
    "        image_B = Image.open(folder_path+\"/\"+str(image_files[1]))\n",
    "        image_B = image_B.resize((224,224))\n",
    "        img_B_array = np.array(image_B)\n",
    "\n",
    "        #Duas imagens diferentes, mas da mesma pessoa\n",
    "        x_train_a.append(img_A_array)\n",
    "        x_train_b.append(img_B_array)\n",
    "        #Como sao a mesma pessoa, sua label recebe 1\n",
    "        y_train.append(1)\n",
    "        counter += 1\n",
    "    #as proximas 252 imagensA serao as primeiras 252 pessoas\n",
    "    for folder in all_folders[:int(amount/2)]:\n",
    "        counter = 0\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png'))]\n",
    "        image_A = Image.open(folder_path+\"/\"+str(image_files[0]))\n",
    "        image_A = image_A.resize((224,224))\n",
    "        img_A_array = np.array(image_A)\n",
    "        x_train_a.append(img_A_array)\n",
    "        y_train.append(0)\n",
    "        counter += 1\n",
    "    #as proximas 252 imagensB serao as PROXIMAS 252 pessoas\n",
    "    for folder in all_folders[int(amount/2):amount]:\n",
    "        counter = 0\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png'))]\n",
    "        image_B = Image.open(folder_path+\"/\"+str(image_files[0]))\n",
    "        image_B = image_B.resize((224,224))\n",
    "        img_B_array = np.array(image_B)\n",
    "        x_train_b.append(img_B_array)\n",
    "    return\n",
    "trainBuild(x_train_a, x_train_b, y_train,amount, \"F:/BiasBusters/Black_Male/Black_Male/\")\n",
    "trainBuild(x_train_a, x_train_b, y_train,amount, \"F:/BiasBusters/Black_Female/Black_Female/\")\n",
    "trainBuild(x_train_a, x_train_b, y_train,amount, \"F:/BiasBusters/Asian_Male/Asian_Male/\")\n",
    "trainBuild(x_train_a, x_train_b, y_train,amount, \"F:/BiasBusters/Asian_Female/Asian_Female/\")\n",
    "trainBuild(x_train_a, x_train_b, y_train,amount, \"F:/BiasBusters/White_Male/White_Male/\")\n",
    "trainBuild(x_train_a, x_train_b, y_train,amount, \"F:/BiasBusters/White_Female/White_Female/\")\n",
    "trainBuild(x_train_a, x_train_b, y_train,amount, \"F:/BiasBusters/Indian_Male/Indian_Male/\")\n",
    "trainBuild(x_train_a, x_train_b, y_train,amount, \"F:/BiasBusters/Indian_Female/Indian_Female/\")\n",
    "trainBuild(x_train_a, x_train_b, y_train,amount, \"F:/BiasBusters/Other_Male/Other_Male/\")\n",
    "trainBuild(x_train_a, x_train_b, y_train,amount, \"F:/BiasBusters/Other_Female/Other_Female/\")\n",
    "\n",
    "print(\"Size of x_train_a = \",len(x_train_a))\n",
    "print(\"Size of x_train_b = \",len(x_train_b))\n",
    "print(\"Size of y_train   = \",len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A = np.array(x_train_a)\n",
    "print(X_train_A.shape)\n",
    "X_train_B = np.array(x_train_b)\n",
    "print(X_train_B.shape)\n",
    "# Y_train = np.array(y_train)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = model.predict([X_train_A,X_train_B])\n",
    "print(\"features_train.shape = \"+ str(features_train.shape) + \"y_train = \" + str(y_train))\n",
    "svm = SVC()\n",
    "svm.fit(features_train, y_train)\n",
    "print(\"Done Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_a = []\n",
    "x_test_b = []\n",
    "y_test = []\n",
    "amount = 504   #per ethnicity\n",
    "# no teste pegaremos 504 imagens por etnia de modo que:\n",
    "# 504-755 teremos comparacao entre diferentes imagens da mesma pessoa.            Ex: 504[0] com 504[1]   | 505[0] com 505[1] e assim por diante...\n",
    "# 756-1007 faremos comparacao entre diferentes imagens de pessoas diferentes.    Ex: 504[0] com 756[0] | 505[0] com 757[0] e assim por diante...\n",
    "def testBuild(x_test_a, x_test_b, y_test, amount, path):\n",
    "    #print(\"Size of x_test = \",len(x_test[0]))\n",
    "    all_folders = [f for f in os.listdir(path)]\n",
    "    #print(all_folders)\n",
    "    #                                [504:756]\n",
    "    for folder in all_folders[amount:amount+int(amount/2)]:\n",
    "        counter = 0\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png'))]\n",
    "\n",
    "        #pegaremos a primeira imagem da pasta da pessoa\n",
    "        image_A = Image.open(folder_path+\"/\"+str(image_files[0]))\n",
    "        image_A = image_A.resize((224,224))\n",
    "        img_A_array = np.array(image_A)\n",
    "\n",
    "        #pegaremos a segunda imagem da pasta da pessoa \n",
    "        image_B = Image.open(folder_path+\"/\"+str(image_files[1]))\n",
    "        image_B = image_B.resize((224,224))\n",
    "        img_B_array = np.array(image_B)\n",
    "\n",
    "        #Duas imagens diferentes, mas da mesma pessoa\n",
    "        x_test_a.append(img_A_array)\n",
    "        x_test_b.append(img_B_array)\n",
    "        #Como sao a mesma pessoa, sua label recebe 1\n",
    "        y_test.append(1)\n",
    "        counter += 1\n",
    "    #as proximas 252 imagensA serao as primeiras 252 pessoas\n",
    "    #                                [504:756]\n",
    "    for folder in all_folders[amount:amount+int(amount/2)]:\n",
    "        counter = 0\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png'))]\n",
    "        image_A = Image.open(folder_path+\"/\"+str(image_files[0]))\n",
    "        image_A = image_A.resize((224,224))\n",
    "        img_A_array = np.array(image_A)\n",
    "        x_test_a.append(img_A_array)\n",
    "        y_test.append(0)\n",
    "        counter += 1\n",
    "    #as proximas 252 imagensB serao as PROXIMAS 252 pessoas\n",
    "    #                                [756:1008]\n",
    "    for folder in all_folders[amount+int(amount/2):]:\n",
    "        counter = 0\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png'))]\n",
    "        image_B = Image.open(folder_path+\"/\"+str(image_files[0]))\n",
    "        image_B = image_B.resize((224,224))\n",
    "        img_B_array = np.array(image_B)\n",
    "        x_test_b.append(img_B_array)\n",
    "    return\n",
    "testBuild(x_test_a, x_test_b, y_test,amount, \"F:/BiasBusters/Black_Male/Black_Male/\")\n",
    "testBuild(x_test_a, x_test_b, y_test,amount, \"F:/BiasBusters/Black_Female/Black_Female/\")\n",
    "testBuild(x_test_a, x_test_b, y_test,amount, \"F:/BiasBusters/Asian_Male/Asian_Male/\")\n",
    "testBuild(x_test_a, x_test_b, y_test,amount, \"F:/BiasBusters/Asian_Female/Asian_Female/\")\n",
    "testBuild(x_test_a, x_test_b, y_test,amount, \"F:/BiasBusters/White_Male/White_Male/\")\n",
    "testBuild(x_test_a, x_test_b, y_test,amount, \"F:/BiasBusters/White_Female/White_Female/\")\n",
    "testBuild(x_test_a, x_test_b, y_test,amount, \"F:/BiasBusters/Indian_Male/Indian_Male/\")\n",
    "testBuild(x_test_a, x_test_b, y_test,amount, \"F:/BiasBusters/Indian_Female/Indian_Female/\")\n",
    "testBuild(x_test_a, x_test_b, y_test,amount, \"F:/BiasBusters/Other_Male/Other_Male/\")\n",
    "testBuild(x_test_a, x_test_b, y_test,amount, \"F:/BiasBusters/Other_Female/Other_Female/\")\n",
    "\n",
    "print(\"Size of x_test_a = \",len(x_test_a))\n",
    "print(\"Size of x_test_b = \",len(x_test_b))\n",
    "print(\"Size of y_test   = \",len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_A = np.array(x_test_a)\n",
    "print(X_test_A.shape)\n",
    "X_test_B = np.array(x_test_b)\n",
    "print(X_test_B.shape)\n",
    "# Y_test = np.array(y_test)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = model.predict([X_test_A,X_test_B])\n",
    "accuracy = svm.score(features_test, y_test)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
